{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0      (milk)     (bread)                0.75                1.00     0.75   \n",
      "1     (bread)      (milk)                1.00                0.75     0.75   \n",
      "\n",
      "   confidence  lift  leverage  conviction  zhangs_metric  \n",
      "0        1.00   1.0       0.0         inf            0.0  \n",
      "1        0.75   1.0       0.0         1.0            0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICRO\\AppData\\Local\\Temp\\ipykernel_16976\\569219870.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  onehot = onehot.applymap(lambda x: 1 if x > 0 else 0)\n",
      "c:\\Users\\MICRO\\Downloads\\JOML\\ML Model Deployment\\Data-Science-Diploma\\Machine-Learning\\Association\\env\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "dataset = {'TransactionID': [1, 1, 2, 2, 3, 3, 4, 4],\n",
    "           'Items': ['milk', 'bread', 'bread', 'butter', 'milk', 'bread', 'milk', 'bread']}\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Convert the dataset into a one-hot encoded format\n",
    "onehot = (df.groupby(['TransactionID', 'Items'])['Items']\n",
    "          .count().unstack().reset_index().fillna(0)\n",
    "          .set_index('TransactionID'))\n",
    "\n",
    "# Convert the one-hot encoded dataset into a boolean format\n",
    "onehot = onehot.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(onehot, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Set a minimum support and confidence threshold. For example, let's set the minimum support to 30% and the minimum confidence to 70%.\n",
    "- Step 2: Generate the list of all unique items (movies) in the dataset. This will be our initial set of frequent 1-itemsets.\n",
    "- Step 3: Calculate the support for each item in the 1-itemset. Support is the percentage of transactions that contain the item.\n",
    "- Step 4: Prune the 1-itemset by removing items with support less than the minimum support threshold.\n",
    "- Step 5: Generate the candidate 2-itemsets by joining the frequent 1-itemsets.\n",
    "- Step 6: Calculate the support for each candidate 2-itemset.\n",
    "- Step 7: Prune the candidate 2-itemset by removing itemsets with support less than the minimum support threshold.\n",
    "- Step 8: Generate candidate 3-itemsets by joining the frequent 2-itemsets.\n",
    "- Step 9: Calculate the support for each candidate 3-itemset.\n",
    "- Step 10: Prune the candidate 3-itemset by removing itemsets with support less than the minimum support threshold.\n",
    "- Step 11: Generate candidate k-itemsets by joining the frequent (k-1)-itemsets.\n",
    "- Step 12: Repeat steps 6 to 10 until no more frequent k-itemsets can be generated.\n",
    "- Step 13: Generate association rules from the frequent itemsets and calculate their confidence.\n",
    "- Step 14: Prune the association rules by removing rules with confidence less than the minimum confidence threshold.\n",
    "- Step 15: The remaining association rules are the final result of the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('movie_recommendation.csv',usecols=['Transaction ID','Movies Purchased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [ ]\n",
    "for i in range(len(data)):\n",
    "    observations.append([str(data.values[i,j]) for j in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "associations = apriori(observations, min_length = 2, min_support = 0.0045,min_confidence = 0.2, min_lift = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(associations)):\n\u001b[1;32m----> 5\u001b[0m     result\u001b[39m.\u001b[39;49mappend(\u001b[39m'\u001b[39;49m\u001b[39mRULE:\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(associations[i][\u001b[39m0\u001b[39;49m]))\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mSUPPORT:\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(result[i][\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "associations = list(associations)\n",
    "\n",
    "result = []\n",
    "for i in range(0,len(associations)):\n",
    "    result.append('RULE:\\t'+ str(associations[i][0]))+'\\nSUPPORT:\\t'+str(result[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RULE:\tfrozenset({'1', 'The Shawshank Redemption, Inception'})\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
